---
layout: archive
title: "Home"
permalink: /
classes: wide
---

<div class="notice--info">
  <strong>Hi, Iâ€™m Donald Loveland</strong> â€” a fourth-year Ph.D. candidate in Computer Science at the University of Michigan, Ann Arbor, advised by <a href="https://gemslab.github.io">Danai Koutra</a>.

  <br><br>
  My research focuses on improving the <strong>interpretability, robustness, and fairness of graph neural networks (GNNs)</strong>. I aim to identify structural mismatches and harmful biases in graph-based learning systems and develop methods that generalize across distributional shifts.

  <br><br>
  Iâ€™m supported by an <a href="https://cse.engin.umich.edu/people/honors-and-awards/awarded-graduate-fellowships/">NSF Graduate Research Fellowship</a> and a <a href="https://rackham.umich.edu/funding/funding-types/rackham-merit-fellowship-program/">Rackham Merit Fellowship</a>.
</div>

---

### Research Interests

- Fairness and interpretability in graph machine learning 
- The impact of structure on graph machine learning performance 
- Efficient recommender systems 
- Integration of GNNs and LLMs 

---

### Recent News

- <strong>May 2025</strong>: Our paper on weight decay in collaborative filtering and popularity bias accepted at <em>KDD 2025</em>!
- <strong>January 2025</strong>: Our paper on negative sampling and matrix rank for collaborative filtering accepted at <em>WWW 2025</em>!
- <strong>December 2024</strong>: Our paper on GNNs for combinatorial optimization accepted at <em>AAMAS 2025</em>!
- <strong>December 2024</strong>: Our paper on GNN fairness under heterophily is accepted at <em>SDM 2025</em>!
- <strong>Summer 2024</strong>: Started as a research intern at Snap Inc. (UMAP team), exploring rank-based training acceleration for recommender systems.

---

### ðŸ’¼ Experience

During the **Summer of 2024**, I interned at **Snap Inc.** on the User Modeling and Personalization (UMAP) team, mentored by Clark Ju, Tong Zhao, and Neil Shah. Our work studied how to accelerate recommender system training by analyzing matrix rank in embedding tables.

In the two prior summers, I interned at **MIT Lincoln Laboratory**, where I worked on bipartite assignment optimization using GNNs and network editing through GNNs.

Before my Ph.D., I was an applied machine learning researcher at **Lawrence Livermore National Laboratory**, where I worked on:
- Molecular graph classification and regression
- Inverse design and generative modeling of molecular structures
- Post-hoc interpretability for graph neural networks
- Vision-based model introspection using generative counterfactuals

---

### Mentoring

As the first in my family to attend college, I deeply value mentorship and community support in academia.

- As an undergraduate, I mentored first-generation students as part of peer mentoring programs.
- At Lawrence Livermore, I helped facilitate a two-week <a href="https://www.llnl.gov/news/lockdown-doesnt-hinder-annual-data-science-challenge">machine learning challenge</a> for students, focusing on applied research skills.
- In graduate school, Iâ€™ve continued to support and mentor undergraduate researchers in CS.

Please feel free to reach out if you ever want to chat about research, navigating academia, or building a career in machine learning!

